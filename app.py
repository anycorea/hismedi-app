# -*- coding: utf-8 -*-
import os, re, html
from pathlib import Path
from typing import List
from urllib.parse import quote

import pandas as pd
import streamlit as st
from sqlalchemy import text, create_engine
from pypdf import PdfReader

# ------------------------------------------------------------
# í˜ì´ì§€/ë ˆì´ì•„ì›ƒ
# ------------------------------------------------------------
st.set_page_config(page_title="â˜…â˜…â˜… HISMEDI ì¸ì¦ ì¤€ë¹„ â˜…â˜…â˜…", layout="wide")
st.markdown("# HISMEDI - ì§€ì¹¨/QnA/ê·œì •")
# í•˜ì´ë¼ì´íŠ¸ ìƒ‰(ì„ íƒ)
st.markdown("<style> mark { background: #ffe2a8; } </style>", unsafe_allow_html=True)

# ------------------------------------------------------------
# DB ì—°ê²° ìœ í‹¸
# ------------------------------------------------------------
def _load_database_url() -> str:
    url = st.secrets.get("DATABASE_URL") or os.getenv("DATABASE_URL")
    if not url:
        st.error("DATABASE_URLì´ ì—†ìŠµë‹ˆë‹¤. `.streamlit/secrets.toml` ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•´ ì£¼ì„¸ìš”.")
        st.stop()
    return str(url).strip()

def _ensure_psycopg_url(url: str) -> str:
    u = url
    if u.startswith("postgresql://"):
        u = u.replace("postgresql://", "postgresql+psycopg://", 1)
    if u.startswith("postgres://"):
        u = u.replace("postgres://", "postgresql+psycopg://", 1)
    if "sslmode=" not in u:
        u += ("&" if ("?" in u) else "?") + "sslmode=require"
    return u

@st.cache_resource(show_spinner="DB ì—”ì§„ ìƒì„± ì¤‘...")
def get_engine():
    url = _ensure_psycopg_url(_load_database_url())
    return create_engine(
        url,
        connect_args={"options": "-c statement_cache_mode=none"},
        pool_pre_ping=True,
    )

def _list_columns(eng, table):
    sql = text("""
        select column_name
        from information_schema.columns
        where table_schema='public' and table_name=:t
        order by ordinal_position
    """)
    with eng.begin() as con:
        return [r[0] for r in con.execute(sql, {"t": table}).fetchall()]

def _table_exists(eng, table):
    with eng.begin() as con:
        return con.execute(text("select to_regclass(:q)"),
                           {"q": f"public.{table}"}).scalar() is not None

def _pick_table(eng, prefer_first: List[str]):
    for t in prefer_first:
        if _table_exists(eng, t):
            return t
    return None

def search_table_any(eng, table, keywords, columns=None, limit=500):
    """ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ì „ë¶€(AND)ë¥¼, ì§€ì •ëœ ì»¬ëŸ¼ë“¤(OR)ì—ì„œ ê²€ìƒ‰"""
    kw_list = [k.strip() for k in str(keywords).split() if k.strip()]
    if not kw_list:
        return pd.DataFrame()
    cols = columns or _list_columns(eng, table)
    where_parts, params = [], {}
    for i, kw in enumerate(kw_list):
        ors = " OR ".join([f'"{c}" ILIKE :kw{i}' for c in cols])
        where_parts.append(f"({ors})")
        params[f"kw{i}"] = f"%{kw}%"
    where_sql = " AND ".join(where_parts)
    sql = text(f'SELECT * FROM "{table}" WHERE {where_sql} LIMIT :lim')
    params["lim"] = int(limit)
    with eng.begin() as con:
        return pd.read_sql_query(sql, con, params=params)

def run_select_query(eng, sql_text, params=None):
    q = (sql_text or "").strip().rstrip(";")
    if not re.match(r"^(select|with|explain)\b", q, re.I):
        raise ValueError("SELECT/WITH/EXPLAINë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    with eng.begin() as con:
        return pd.read_sql_query(text(q), con, params=params or {})

# ------------------------------------------------------------
# PDF ì¸ë±ì‹±/ê²€ìƒ‰ (ìŠ¤í‚¤ë§ˆ ìë™ êµì • í¬í•¨)
# ------------------------------------------------------------
REQUIRED_REG_COLUMNS = ["id", "filename", "page", "text", "file_mtime", "me"]

def ensure_reg_table(eng):
    """regulations ìŠ¤í‚¤ë§ˆ ì ê²€ â†’ ë¶€ì¡±/ë¶ˆì¼ì¹˜ ì‹œ ì•ˆì „í•˜ê²Œ ì¬ìƒì„±"""
    with eng.begin() as con:
        try:
            con.execute(text("create extension if not exists pg_trgm"))
        except Exception:
            pass

        exists = con.execute(
            text("select to_regclass('public.regulations')")
        ).scalar() is not None

        recreate = False
        if exists:
            cols = [r[0] for r in con.execute(text("""
                select column_name
                from information_schema.columns
                where table_schema='public' and table_name='regulations'
            """)).fetchall()]
            need = set(REQUIRED_REG_COLUMNS) - set(cols)
            if need:
                recreate = True
        else:
            recreate = True

        if recreate:
            con.execute(text("drop table if exists regulations cascade"))
            con.execute(text("""
                create table regulations (
                  id bigserial primary key,
                  filename text not null,
                  page int not null,
                  text text not null,
                  file_mtime bigint not null,
                  me text
                )
            """))

        con.execute(text("create index if not exists idx_reg_file on regulations(filename)"))
        con.execute(text("create index if not exists idx_reg_me   on regulations(me)"))
        try:
            con.execute(text("create index if not exists idx_reg_text_trgm on regulations using gin (text gin_trgm_ops)"))
        except Exception:
            pass

def _clean_text(s: str) -> str:
    s = s or ""
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def index_pdfs(eng, folder: Path):
    """í´ë”ì˜ PDFë¥¼ íŒŒì¼ë³„ ìµœì‹  mtime ê¸°ì¤€ìœ¼ë¡œ incremental ì¸ë±ì‹±
       ('.ipynb_checkpoints' ê²½ë¡œëŠ” ìë™ ì œì™¸)"""
    ensure_reg_table(eng)
    pdfs = sorted(folder.rglob("*.pdf"))
    if not pdfs:
        return {"indexed": 0, "skipped": 0, "errors": 0, "files": []}

    indexed = skipped = errors = 0
    done_files = []

    with eng.begin() as con:
         # >>> ê¸°ì¡´ì— ë“¤ì–´ê°„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ì‹¹ ì •ë¦¬(í•œ ë²ˆ ì‹¤í–‰í•´ë‘ë©´ ì¢‹ìŒ)
        con.execute(text(r"delete from regulations where filename ~* '(^|[\\/])\.ipynb_checkpoints([\\/]|$)'"))

        for f in pdfs:
            # === ì²´í¬í¬ì¸íŠ¸ í´ë” ì œì™¸ ===
            if any(part == ".ipynb_checkpoints" for part in f.parts):
                skipped += 1
                try:
                    fn_skip = str(f.relative_to(folder))
                except Exception:
                    fn_skip = str(f)
                done_files.append((fn_skip, "skip(checkpoints)"))
                continue
            # =========================

            try:
                fn = str(f.relative_to(folder))
            except Exception:
                fn = str(f)
            mt = int(f.stat().st_mtime)

            # ì´ë¯¸ ìµœì‹ ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            row = con.execute(
                text("select count(*) from regulations where filename=:fn and file_mtime=:mt"),
                {"fn": fn, "mt": mt},
            ).scalar()
            if row and row > 0:
                skipped += 1
                done_files.append((fn, "skip"))
                continue

            # ê¸°ì¡´ íŒŒì¼ ë ˆì½”ë“œ ì œê±° í›„ ì¬ì ì¬
            con.execute(text("delete from regulations where filename=:fn"), {"fn": fn})

            # PDF ì½ê¸°
            try:
                reader = PdfReader(str(f))
            except Exception as e:
                errors += 1
                done_files.append((fn, f"open-fail: {type(e).__name__}"))
                continue

            rows = []
            for pno, page in enumerate(reader.pages, start=1):
                try:
                    txt = page.extract_text() or ""
                except Exception:
                    txt = ""
                txt = _clean_text(txt)
                if not txt:
                    continue
                rows.append({"filename": fn, "page": pno, "text": txt, "file_mtime": mt, "me": None})

            if rows:
                df = pd.DataFrame(rows)
                df.to_sql("regulations", con, if_exists="append", index=False)
                indexed += 1
                done_files.append((fn, f"indexed {len(rows)}p"))
            else:
                done_files.append((fn, "no-text"))

    return {"indexed": indexed, "skipped": skipped, "errors": errors, "files": done_files}

def make_snippet(text_: str, kw_list: List[str], width: int = 160) -> str:
    """ê°€ì¥ ë¨¼ì € ë§ì€ í‚¤ì›Œë“œ ì£¼ë³€ìœ¼ë¡œ ìŠ¤ë‹ˆí« ìƒì„±"""
    if not text_:
        return ""
    low = text_.lower()
    pos = -1
    hit = ""
    for k in kw_list:
        k = k.lower()
        i = low.find(k)
        if i != -1 and (pos == -1 or i < pos):
            pos = i
            hit = k
    if pos == -1:
        return text_[:width] + ("..." if len(text_) > width else "")
    start = max(0, pos - width // 2)
    end = min(len(text_), pos + len(hit) + width // 2)
    prefix = "..." if start > 0 else ""
    suffix = "..." if end < len(text_) else ""
    return f"{prefix}{text_[start:end]}{suffix}"

def highlight_html(src_text: str, kw_list: List[str], width: int = 200) -> str:
    """ìŠ¤ë‹ˆí«ì„ ë§Œë“  ë’¤, í‚¤ì›Œë“œë¥¼ <mark>ë¡œ ê°•ì¡°í•´ HTMLë¡œ ë°˜í™˜."""
    snippet = make_snippet(src_text, kw_list, width=width)
    esc = html.escape(snippet)
    for k in sorted({k for k in kw_list if k}, key=len, reverse=True):
        pattern = re.compile(re.escape(k), re.IGNORECASE)
        esc = pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", esc)
    return esc

def search_regs(eng, keywords: str, filename_like: str = "", limit: int = 500, hide_ipynb_chk: bool = True):
    """PDF ë³¸ë¬¸ ê²€ìƒ‰. ê¸°ë³¸ê°’ìœ¼ë¡œ .ipynb_checkpoints ê²°ê³¼ë¥¼ ìˆ¨ê¹€."""
    kw_list = [k.strip() for k in str(keywords).split() if k.strip()]
    if not kw_list:
        return pd.DataFrame()

    where_parts, params = [], {}

    # ë³¸ë¬¸ í‚¤ì›Œë“œ AND
    for i, kw in enumerate(kw_list):
        where_parts.append(f"(text ILIKE :kw{i})")
        params[f"kw{i}"] = f"%{kw}%"

    # íŒŒì¼ëª… í•„í„°(ì„ íƒ)
    if filename_like.strip():
        where_parts.append("filename ILIKE :fn")
        params["fn"] = f"%{filename_like.strip()}%"

    # >>> .ipynb_checkpoints ìˆ¨ê¸°ê¸°(ì •ê·œì‹)
    # ìŠ¬ë˜ì‹œ/ë°±ìŠ¬ë˜ì‹œ ë‘˜ ë‹¤ ê²½ë¡œ êµ¬ë¶„ìë¡œ ì¸ì‹
    if hide_ipynb_chk:
        where_parts.append(r"(filename !~* '(^|[\\/])\.ipynb_checkpoints([\\/]|$)')")

    where_sql = " AND ".join(where_parts)
    sql = text(f"""
        select filename, page, me, text
        from regulations
        where {where_sql}
        order by filename, page
        limit :lim
    """)
    params["lim"] = int(limit)
    with eng.begin() as con:
        return pd.read_sql_query(sql, con, params=params)

# ------------------------------------------------------------
# ì—°ê²° ë°°ë„ˆ
# ------------------------------------------------------------
eng = get_engine()
try:
    with eng.begin() as con:
        user, port, now = con.execute(
            text("select current_user, inet_server_port(), now()")
        ).one()
    st.success(f"DB ì—°ê²° OK | user={user} | port={port} | time={now}")
except Exception as e:
    st.exception(e); st.stop()

# ------------------------------------------------------------
# íƒ­ UI
# ------------------------------------------------------------
tab_main, tab_qna, tab_pdf = st.tabs(
    ["Main ì¡°íšŒ", "QnA ì¡°íšŒ", "PDF ê²€ìƒ‰"]
)

# --------------------- Main ì¡°íšŒ (í•„í„° í¬í•¨ í†µí•©ê²€ìƒ‰) ----------------------------
with tab_main:
    st.subheader("Main ì¡°íšŒ")

    main_table = _pick_table(eng, ["main_v", "main_raw"]) or "main_raw"
    all_cols = _list_columns(eng, main_table)

    # --- (ì„ íƒ) ìë™ ì»¬ëŸ¼ ë§¤í•‘: ì‹¤ì œ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í›„ë³´ì—ì„œ 1ê°œ ìë™ ì„ íƒ
    CAND_PLACE  = ["ì¡°ì‚¬ì¥ì†Œ", "ì¥ì†Œ", "location", "place", "ë¶€ì„œ", "dept"]
    CAND_TARGET = ["ì¡°ì‚¬ëŒ€ìƒ", "ëŒ€ìƒ", "target", "role", "ì§êµ°", "ì§ë¬´"]

    def _pick_col(cands):
        for c in cands:
            if c in all_cols:
                return c
        return None

    col_place  = _pick_col(CAND_PLACE)
    col_target = _pick_col(CAND_TARGET)

    # --- ê²€ìƒ‰ ëŒ€ìƒ ì—´ ì„ íƒ(ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
    kw = st.text_input("ì „ì²´ í‚¤ì›Œë“œ (ê³µë°±=AND Â· ì„ íƒëœ ì—´ì—ì„œ ê²€ìƒ‰)", "", key="main_kw")

    mode = st.radio(
        "ì „ì²´ í‚¤ì›Œë“œ ê²€ìƒ‰ ëŒ€ìƒ",
        ["ì „ì²´ ì—´", "íŠ¹ì • ì—´ ì„ íƒ", "MEë§Œ"],
        horizontal=True,
        key="main_mode"
    )

    if mode == "íŠ¹ì • ì—´ ì„ íƒ":
        sel_cols = st.multiselect("ê²€ìƒ‰í•  ì—´ ì„ íƒ", options=all_cols, default=all_cols, key="main_cols")
    elif mode == "MEë§Œ":
        sel_cols = ["ME"] if "ME" in all_cols else all_cols[:1]
        if "ME" not in all_cols:
            st.info("ME ì¹¼ëŸ¼ì´ ì—†ì–´ ì²« ì¹¼ëŸ¼ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    else:
        sel_cols = None  # ì „ì²´ ì—´

    st.divider()

    # --- í•„í„° ì¡°ê±´(ë‹¨ì–´ 1ê°œ, ë¶€ë¶„ì¼ì¹˜) : ì¡°ì‚¬ì¥ì†Œ / ì¡°ì‚¬ëŒ€ìƒ
    c1, c2 = st.columns(2)
    with c1:
        place_word = st.text_input(
            f"ì¡°ì‚¬ì¥ì†Œ í•„í„°(ë‹¨ì–´ 1ê°œ Â· ë¶€ë¶„ì¼ì¹˜) [{col_place or 'ì»¬ëŸ¼ì—†ìŒ'}]",
            "",
            key="flt_place_one",
            help="ì˜ˆ: ì›ë¬´  â†’ ì¡°ì‚¬ì¥ì†Œì— 'ì›ë¬´'ê°€ í¬í•¨ëœ í–‰ë§Œ"
        )
    with c2:
        target_word = st.text_input(
            f"ì¡°ì‚¬ëŒ€ìƒ í•„í„°(ë‹¨ì–´ 1ê°œ Â· ë¶€ë¶„ì¼ì¹˜) [{col_target or 'ì»¬ëŸ¼ì—†ìŒ'}]",
            "",
            key="flt_target_one",
            help="ì˜ˆ: ê°„í˜¸ì‚¬ â†’ ì¡°ì‚¬ëŒ€ìƒì— 'ê°„í˜¸ì‚¬'ê°€ í¬í•¨ëœ í–‰ë§Œ"
        )

    # --- ì¶”ê°€ ì»¬ëŸ¼ í•„í„°: í•„ìš”í•œ ì»¬ëŸ¼ì„ ì„ íƒí•˜ê³ , ê°ìì— ë¶€ë¶„ì¼ì¹˜ ë‹¨ì–´ 1ê°œ ì…ë ¥
    st.markdown("**ì¶”ê°€ ì»¬ëŸ¼ í•„í„°(ì„ íƒ)** â€” í•„ìš”í•œ ì»¬ëŸ¼ì„ ì„ íƒí•˜ê³  ê°ì ê²€ìƒ‰ì–´(ë¶€ë¶„ì¼ì¹˜)ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    extra_cols = st.multiselect(
        "ì¶”ê°€ë¡œ í•„í„°í•  ì»¬ëŸ¼ë“¤ ì„ íƒ",
        options=[c for c in all_cols if c not in {col_place, col_target}],
        default=[],
        key="extra_cols"
    )
    extra_terms = {}
    if extra_cols:
        cols_box = st.columns(min(3, len(extra_cols)))
        for i, c in enumerate(extra_cols):
            with cols_box[i % len(cols_box)]:
                extra_terms[c] = st.text_input(f"{c} í¬í•¨ ë‹¨ì–´", "", key=f"extra_term_{c}")

    limit = st.number_input("ìµœëŒ€ í–‰ìˆ˜", 1, 10000, 1000, step=100, key="main_lim2")

    if st.button("ê²€ìƒ‰", key="main_search2"):
        where_parts, params = [], {}

        # 1) ì¡°ì‚¬ì¥ì†Œ í•„í„°
        if place_word.strip() and col_place:
            where_parts.append(f'"{col_place}" ILIKE :place')
            params["place"] = f"%{place_word.strip()}%"

        # 2) ì¡°ì‚¬ëŒ€ìƒ í•„í„°
        if target_word.strip() and col_target:
            where_parts.append(f'"{col_target}" ILIKE :target')
            params["target"] = f"%{target_word.strip()}%"

        # 3) ì¶”ê°€ ì»¬ëŸ¼ í•„í„°(ê° ì»¬ëŸ¼ë³„ ë¶€ë¶„ì¼ì¹˜)
        for c, term in (extra_terms or {}).items():
            t = (term or "").strip()
            if t:
                pkey = f"extra_{len(params)}"
                where_parts.append(f'"{c}" ILIKE :{pkey}')
                params[pkey] = f"%{t}%"

        # 4) ì „ì²´ í‚¤ì›Œë“œ(ê³µë°±=AND) â€” ì„ íƒëœ ì—´(sel_cols) ëŒ€ìƒ, ì—†ìœ¼ë©´ ì „ì²´ ì—´ ëŒ€ìƒ
        if kw.strip():
            tokens = [t for t in kw.split() if t.strip()]
            cols_for_kw = sel_cols if sel_cols else all_cols
            for i, t in enumerate(tokens):
                # ì„ íƒëœ(ë˜ëŠ” ì „ì²´) ì—´ ì¤‘ ì–´ëŠ í•˜ë‚˜ì—ë¼ë„ í¬í•¨ë˜ë©´ í†µê³¼
                ors = " OR ".join([f'CAST("{c}" AS TEXT) ILIKE :kw{i}' for c in cols_for_kw])
                where_parts.append(f"({ors})")
                params[f"kw{i}"] = f"%{t}%"

        where_sql = " AND ".join(where_parts) if where_parts else "TRUE"
        sql = text(f'SELECT * FROM "{main_table}" WHERE {where_sql} LIMIT :lim')
        params["lim"] = int(limit)

        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            with eng.begin() as con:
                df = pd.read_sql_query(sql, con, params=params)

        st.write(f"ê²°ê³¼: {len(df):,}ê±´")
        if df.empty:
            st.info("ê²°ê³¼ ì—†ìŒ")
        else:
            st.dataframe(df, use_container_width=True, height=520)

            # (ì„ íƒ) CSV ë‹¤ìš´ë¡œë“œ
            csv = df.to_csv(index=False).encode("utf-8-sig")
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", csv, "main_filtered_search.csv", "text/csv")

    else:
        st.caption("ìˆœì„œ: (í•„ìš”ì‹œ) ì¡°ì‚¬ì¥ì†Œ/ì¡°ì‚¬ëŒ€ìƒ/ì¶”ê°€ ì»¬ëŸ¼ í•„í„° ì…ë ¥ â†’ ì „ì²´ í‚¤ì›Œë“œ ì…ë ¥ â†’ [ê²€ìƒ‰] í´ë¦­")

# --------------------- QnA ì¡°íšŒ -----------------------------
with tab_qna:
    st.subheader("QnA ì¡°íšŒ")
    qna_table = _pick_table(eng, ["qna_v", "qna_raw"]) or "qna_raw"
    kw_q = st.text_input("í‚¤ì›Œë“œ (ê³µë°±=AND)", "", key="qna_kw")
    limit_q = st.number_input("ìµœëŒ€ í–‰ìˆ˜", 1, 5000, 500, step=100, key="qna_lim")

    if st.button("ê²€ìƒ‰", key="qna_search") and kw_q.strip():
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            df = search_table_any(eng, qna_table, kw_q, columns=None, limit=limit_q)
        st.write(f"ê²°ê³¼: {len(df):,}ê±´")

        if df.empty:
            st.info("ê²°ê³¼ ì—†ìŒ")
        else:
            st.dataframe(df, use_container_width=True, height=520)

    else:
        st.caption("í•„ìš” ì‹œ â€˜ì„ì‹œ SQLâ€™ì—ì„œ ì§ì ‘ SELECT ì‹¤í–‰ ê°€ëŠ¥.")


# --------------------- PDF ê²€ìƒ‰ (HTTP ì„œë²„ ë°©ì‹ ê³ ì •) -------
with tab_pdf:
    st.subheader("PDF ê²€ìƒ‰")

    # 1) ì¸ë±ì‹± ê´€ë ¨
    default_folder = str((Path.cwd() / "PDFs").resolve())
    folder_str = st.text_input("PDF í´ë” ê²½ë¡œ", default_folder, key="pdf_folder")
    folder = Path(folder_str)

    cols1 = st.columns(3)
    with cols1[0]:
        if st.button("ì¸ë±ìŠ¤ ê°±ì‹ ", key="pdf_reindex"):
            if not folder.exists():
                st.error(f"í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {folder}")
            else:
                with st.spinner("PDF ì¸ë±ì‹± ì¤‘... (ì²˜ìŒì€ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                    rep = index_pdfs(eng, folder)
                st.success(f"ì¸ë±ìŠ¤ ì™„ë£Œ | indexed={rep['indexed']} | skipped={rep['skipped']} | errors={rep['errors']}")
                with st.expander("ìƒì„¸ ë¡œê·¸ ë³´ê¸°"):
                    for fn, stat in rep["files"]:
                        st.write(f"- {fn}: {stat}")

    st.markdown(
        "ğŸ”— **ë§í¬ ë°©ì‹: HTTP ì„œë²„(ê¶Œì¥)** &nbsp;&nbsp;"
        "`cd /d D:\\Anaconda\\PDFs && python -m http.server 8010` ì„ ë³„ë„ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”."
    )
    http_base = st.text_input(
        "HTTP ì„œë²„ ì£¼ì†Œ",
        value="http://localhost:8010",
        key="pdf_http_base",
        help="ë‹¤ë¥¸ PCì—ì„œ ì—´ë ¤ë©´ http://<ë‚´PC IP>:8010 ë¡œ ë³€ê²½ (ì˜ˆ: http://192.168.0.23:8010)"
    )
    base_http = http_base.rstrip("/")

    st.divider()

    # 2) ê²€ìƒ‰ ì¡°ê±´
    kw_pdf = st.text_input("í‚¤ì›Œë“œ (ê³µë°±=AND)", "", key="pdf_kw")
    fn_like = st.text_input("íŒŒì¼ëª… í•„í„°(ì„ íƒ)", "", key="pdf_fn")
    limit_pdf = st.number_input("ìµœëŒ€ ê²°ê³¼", 1, 5000, 500, step=100, key="pdf_lim")

    # 3) ê²€ìƒ‰ ì‹¤í–‰
    if st.button("ê²€ìƒ‰", key="pdf_search") and kw_pdf.strip():
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            df = search_regs(eng, kw_pdf, filename_like=fn_like, limit=int(limit_pdf))
        st.write(f"ê²°ê³¼: {len(df):,}ê±´")
        if df.empty:
            st.info("ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            kw_list = [k.strip() for k in kw_pdf.split() if k.strip()]

            # HTTP ë§í¬ ë§Œë“¤ê¸° (ìƒëŒ€ê²½ë¡œ â†’ URL ì¸ì½”ë”©, ìŠ¬ë˜ì‹œ ìœ ì§€)
            def make_href(row):
                rel_web = str(row["filename"]).replace("\\", "/")
                rel_enc = quote(rel_web, safe="/")  # '/'ëŠ” ìœ ì§€í•˜ê³  í•œê¸€/ê³µë°±ë§Œ ì¸ì½”ë”©
                return f"{base_http}/{rel_enc}#page={int(row['page'])}"

            view = df[["filename", "page", "me"]].copy()
            view["open"] = df.apply(make_href, axis=1)

            st.dataframe(
                view,
                use_container_width=True,
                height=520,
                column_config={"open": st.column_config.LinkColumn("ì—´ê¸°", display_text="ì—´ê¸°")},
            )

            # ë¯¸ë¦¬ë³´ê¸°(í•˜ì´ë¼ì´íŠ¸ + iframe)
            st.caption("í–‰ì„ ì„ íƒí•´ ë³¸ë¬¸ ìŠ¤ë‹ˆí«ê³¼ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            with st.expander("í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° & ë¬¸ì„œ ë³´ê¸° (ì„ íƒí•œ 1ê±´)"):
                idx = st.number_input(
                    "í–‰ ë²ˆí˜¸(0ë¶€í„°)",
                    min_value=0, max_value=len(df) - 1, value=0, step=1,
                    key="pdf_preview_idx"
                )
                row = df.iloc[int(idx)]
                st.write(f"**íŒŒì¼**: {row['filename']}  |  **í˜ì´ì§€**: {int(row['page'])}  |  **ME**: {row.get('me') or '-'}")
                st.markdown(highlight_html(row["text"], kw_list, width=200), unsafe_allow_html=True)

                # iframe ë¯¸ë¦¬ë³´ê¸° (HTTP ì„œë²„ê°€ ì¼œì ¸ ìˆì–´ì•¼ ë³´ì„)
                href = make_href(row)
                st.components.v1.html(
                    f'<iframe src="{href}" style="width:100%; height:720px;" frameborder="0"></iframe>',
                    height=740,
                )

    else:
        st.caption("ë¨¼ì € [ì¸ë±ìŠ¤ ê°±ì‹ ]ì„ í•œ ë²ˆ ìˆ˜í–‰í•´ì•¼ ê²€ìƒ‰ì´ ì˜ ë™ì‘í•©ë‹ˆë‹¤.")